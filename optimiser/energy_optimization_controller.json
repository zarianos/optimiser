# optimiser/energy_optimization_controller.py
# SPDX-License-Identifier: Apache-2.0
import os, asyncio, json, glob, time, math, typing, pathlib
from pathlib import Path

from exporter            import Exporter
from software_actuator   import SoftwareActuator
from decision_engine      import HierarchicalAgent, HierMem

try:                           # nice colourised tables if available
    from rich.console import Console
    from rich.table   import Table
    console = Console()
except ImportError:
    console = None

# ------------------------------------------------------------------
ACTION_DO_NOTHING              = 0
ACTION_CONSOLIDATE             = 1
ACTION_DEFRAGMENT              = 2
ACTION_HARDWARE_TUNE           = 3
ACTION_NAMES = ("DO_NOTHING", "CONSOLIDATE", "DEFRAGMENT", "HARDWARE_TUNE")

SUGGESTION_DIR     = Path("/tmp/k8s_optimizer_suggestions")
TRIGGER_FILE       = Path("/tmp/k8s_optimizer_trigger.signal")

# ------------------------------------------------------------------
class OptimizationController:
    def __init__(self,
                 agent: HierarchicalAgent,
                 data_collector,              # StateBuilder-like
                 memory: HierMem,
                 exporter: Exporter,
                 update_timestep: int = 400):

        self.agent      = agent
        self.sb         = data_collector
        self.memory     = memory
        self.exporter   = exporter
        self.update_ts  = update_timestep
        self.timestep   = 0
        self.cum_saved  = 0.0

        self.v1         = data_collector.v1           # CoreV1Api
        self.actuator   = SoftwareActuator(self.v1)

        SUGGESTION_DIR.mkdir(parents=True, exist_ok=True)
        TRIGGER_FILE.unlink(missing_ok=True)

    # ──────────────────────────────────────────────────────────
    #  REWARD + STATE ENCODING
    # ──────────────────────────────────────────────────────────
    @staticmethod
    def calculate_reward(state: dict) -> float:
        nodes = list(state["nodes"].values())
        if not nodes:
            return 0.0
        # simple reward: negative mean power + balance
        mean_power = sum(n["cpu_power_watts"] for n in nodes)/len(nodes)
        util_var   = math.sqrt(sum((n["cpu_util"]-0.5)**2 for n in nodes)/len(nodes))
        return -mean_power - util_var*50        # tune weights as needed

    def state_vector(self, state):
        return self.sb.to_vector(state)

    # ──────────────────────────────────────────────────────────
    #  ACTION ANALYSIS / EXECUTION
    # ──────────────────────────────────────────────────────────
    def _pretty_state(self, state):
        if not console:   # fallback
            print("Nodes:", ", ".join(state["nodes"]))
            return
        t = Table("Node", "CPU util %", "Power W")
        for n, d in state["nodes"].items():
            t.add_row(n, f"{d['cpu_util']*100:.1f}", f"{d['cpu_power_watts']:.0f}")
        console.print(t)

    def _suggest(self, fam: int, tgt_idx: int, state: dict):
        """Return dict describing what would be done."""
        sug = {"action": fam, "target": None, "reason": ""}
        if fam == ACTION_DO_NOTHING:
            sug["reason"] = "Policy chose no-op"
            return sug

        if fam in (ACTION_CONSOLIDATE, ACTION_DEFRAGMENT):
            nodes = list(state["nodes"])
            if not nodes:
                sug["action"] = ACTION_DO_NOTHING
                sug["reason"] = "No nodes"
                return sug
            node_name = nodes[tgt_idx % len(nodes)]
            sug["target"] = node_name
            sug["reason"] = f"Selected node {node_name}"
        elif fam == ACTION_HARDWARE_TUNE:
            # just attach node label; DaemonSet will apply governor
            nodes = list(state["nodes"])
            node_name = nodes[tgt_idx % len(nodes)]
            sug["target"] = node_name
            sug["reason"] = f"Tuning CPU governor on {node_name}"
        return sug

    async def _execute(self, suggestion, state_before, settle: int):
        fam = suggestion["action"]
        if fam == ACTION_DO_NOTHING:
            return 0.0

        if fam in (ACTION_CONSOLIDATE, ACTION_DEFRAGMENT):
            # pick heaviest (or lightest) pod on that node
            node = suggestion["target"]
            pods_on_node = [p for p in state_before["pods"].values() if p["node"] == node]
            if not pods_on_node:
                return 0.0

            if fam == ACTION_CONSOLIDATE:
                pod = max(pods_on_node, key=lambda p: p["cpu_millicores"])
            else:
                pod = min(pods_on_node, key=lambda p: p["cpu_millicores"])

            self.actuator.evict_pod(pod["name"], pod["namespace"])

        elif fam == ACTION_HARDWARE_TUNE:
            node = suggestion["target"]
            # label patch → node-tuner DaemonSet handles governor
            body = {"metadata": {"labels": {"optimiser/tune": "cpusave"}}}
            self.v1.patch_node(node, body)

        print(f"▷ executed {ACTION_NAMES[fam]} on {suggestion.get('target')}")
        print("  waiting for cluster to settle …")
        await asyncio.sleep(settle)
        return self._power_delta(state_before)

    # Δ power helper
    @staticmethod
    def _power_sum(state):         # Watts
        return sum(n["cpu_power_watts"] for n in state["nodes"].values())

    def _power_delta(self, state_before):
        after = asyncio.run(self.sb.get_cluster_state())   # sync shortcut
        return self._power_sum(state_before) - self._power_sum(after)

    # ──────────────────────────────────────────────────────────
    #  MAIN LOOP
    # ──────────────────────────────────────────────────────────
    async def run_loop_async(self,
                             observation_interval: int = 30,
                             action_settle_time: int = 90):
        print("⚙  controller started; suggestions in", SUGGESTION_DIR)
        while True:
            self.timestep += 1
            state = await self.sb.get_cluster_state()
            if not state["nodes"]:
                await asyncio.sleep(observation_interval)
                continue

            vec  = self.state_vector(state)
            fam, tgt = self.agent.select_action(vec, self.memory)

            suggestion = self._suggest(fam, tgt, state)

            # save suggestion to disk (for manual trigger)
            path = SUGGESTION_DIR / f"sug_{self.timestep}.json"
            path.write_text(json.dumps(suggestion))

            # friendly log
            print(f"\n── cycle {self.timestep} — {time.ctime()} ──")
            self._pretty_state(state)
            print("Suggested:", ACTION_NAMES[suggestion["action"]], suggestion.get("target"))

            # auto-apply if not DO_NOTHING
            delta = await self._execute(suggestion, state, action_settle_time)
            if delta:
                self.cum_saved += delta
                self.exporter.record(before=0, after=0, action_id=fam)  # we set real Watts later
                print(f"Δ power realised: {delta:.1f} W  (cumulative {self.cum_saved:.1f} W)")

            # reward bookkeeping
            r = self.calculate_reward(state)
            self.memory.high.rewards.append(r)
            self.memory.high.is_terminals.append(False)
            self.memory.low.rewards.append(r)
            self.memory.low.is_terminals.append(False)

            # RL update
            if self.timestep % self.update_ts == 0:
                self.agent.update(self.memory)

            await asyncio.sleep(observation_interval)
